{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Functions for Homework 4\n",
    "\n",
    "This notebook contains code to create the complete the clustering needed for homework 4.\n",
    "\n",
    "Please read __ALL__ the comments in the code and the headings. This notebook is NOT intended to be run as a script from top to bottom, although there are some code cells that need to be run first.\n",
    "- The general utility libraries need to be loaded first\n",
    "- Then you need to execute the load data and engineer features code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general utilities\n",
    "# ----------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREP AND PREPROCESSING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the data and engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code you can use to open your pickle file\n",
    "# Read the data and features from the pickle\n",
    "data, discrete_features, continuous_features, ret_cols = pickle.load( open( \"Data/clean_data.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature for the length of a person's credit history at the\n",
    "# time the loan is issued\n",
    "data['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\n",
    "continuous_features.append('cr_hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to use a smaller sample of the data due to time constraints, use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code randomly samples 55% of the rows\n",
    "# change the frac paramter if you want a different % to sample\n",
    "# replace = False insures we won't select the same row twice\n",
    "data=data.sample(frac=.55, replace=False, ).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VERY IMPORTANT STEP\n",
    "You need to define which features to use in the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discrete features you want to use in modeling.\n",
    "# if you want to use all the discrete features, just set discrete_features_touse = discrete_features\n",
    "discrete_features_touse =['purpose', 'term', 'verification_status', 'emp_length', 'home_ownership']\n",
    "\n",
    "# define the continuous features to use in modeling\n",
    "# if you want to use all the continuous features, just set the continuous_features_touse = continuous_features\n",
    "continuous_features_touse = ['loan_amnt', 'funded_amnt','installment','annual_inc','dti','revol_bal','delinq_2yrs','open_acc',\n",
    " 'pub_rec','fico_range_high','fico_range_low','revol_util','cr_hist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the code below if you want to use all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features_touse=discrete_features\n",
    "continuous_features_touse = continuous_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def minMaxScaleContinuous(continuousList):\n",
    "    return pd.DataFrame(MinMaxScaler().fit_transform(data[continuousList])\n",
    "                             ,columns=list(data[continuousList].columns)\n",
    "                             ,index = data[continuousList].index)\n",
    "\n",
    "def createDiscreteDummies(discreteList):\n",
    "    return pd.get_dummies(data[discreteList], dummy_na = True, prefix_sep = \"::\", drop_first = False)\n",
    "\n",
    "def createTransformedData(continuousList,discreteList):\n",
    "    # use this line if you want to scale the continuous features using the MinMaxScaler in the function defined above\n",
    "    continuous = minMaxScaleContinuous(continuousList)\n",
    "\n",
    "    # create numeric dummy features for the discrete features to be used in modeling\n",
    "    discrete = createDiscreteDummies(discreteList)\n",
    "\n",
    "    #concatenate the continuous and discrete features into one dataframe\n",
    "    return pd.concat([continuous, discrete], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Determine Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def determineClusters(transformed_data, range_min, range_max):\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(range_min, range_max)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k, n_jobs=-1)\n",
    "        km = km.fit(transformed_data)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "        \n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and Transform Data\n",
    "\n",
    "- The continuous features are scaled and dummies are created for the discrete features\n",
    "- The dataset is split into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_data = pd.concat([createTransformedData(continuous_features_touse, discrete_features_touse),data[ret_cols]], axis = 1)\n",
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.concat([createTransformedData(continuous_features_touse, discrete_features_touse),data[ret_cols]], axis = 1)\n",
    "#y = data[ret_cols]\n",
    "\n",
    "# create a test and train split of the transformed data\n",
    "X_train, X_test = train_test_split(X, random_state=0, test_size=.3)\n",
    "\n",
    "clusterInput = X_train.iloc[:,:-len(ret_cols)]\n",
    "predictInput = X_test.iloc[:,:-len(ret_cols)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Number of Clusters\n",
    "\n",
    "- Use the Elbow Method to determine proper number of clusters\n",
    "- You can change the range_min and range_max parameters to control the range of clusters that are considered in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determineClusters(clusterInput, range_min=2, range_max=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Kmeans Clustering\n",
    "\n",
    "- Fit the clustering on the training data\n",
    "- The number of clusters parameter comes from the ideal number as determined by the Elbow method above\n",
    "- The other parameters can be adjusted as well. You can find documentation at the link below\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=16, random_state=0, n_jobs=-1).fit(clusterInput)\n",
    "\n",
    "print(\"Sum Squared Distances: \", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Cluster to the Training Data\n",
    "- Create a dataframe of the standard deviations by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterStdDev=X_train.groupby('cluster')[ret_cols].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Clusters for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['cluster'] = kmeans.predict(predictInput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
