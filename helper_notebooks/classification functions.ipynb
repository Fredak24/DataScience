{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Functions for Homeworks 2 & 3\n",
    "\n",
    "This notebook contains code to create the classification models needed for homeworks 2 and 3. It also has code to visualize the performance of the models created.\n",
    "\n",
    "Please read __ALL__ the comments in the code and the headings. This notebook is NOT intended to be run as a script from top to bottom, although there are some code cells that need to be run first.\n",
    "- The general utility libraries need to be loaded first\n",
    "- Then you need to execute the load data and engineer features code cells\n",
    "- Finally, execute the create X and y from the features code cells\n",
    "\n",
    "You can choose to use utilize the grid search implementations for each algorithm listed below using the given RFECV implementation or choose your own feature selection implementation. __OR__ you can choose to implement a pipeline for each of the algorithms using the example pipelines listed in this notebook.\n",
    "\n",
    "The description box above each algorithm contains a link to sklearn's documentation on that algorithm. Please use that for parameter tuning.\n",
    "\n",
    "Please note that some algorithms may be sensitive to scale. Therefore, for those algorithms, you may need to use a scaler (such as the StandardScaler or MinMaxScaler) either before feature selection or within your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general utilities\n",
    "# ----------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREP AND PREPROCESSING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the data and engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code you can use to open your pickle file\n",
    "# Read the data and features from the pickle\n",
    "data, discrete_features, continuous_features, ret_cols = pickle.load( open( \"Data/clean_data.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outcome\n",
    "data[\"default\"] = data.loan_status.isin([\"Charged Off\", \"Default\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature for the length of a person's credit history at the\n",
    "# time the loan is issued\n",
    "data['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\n",
    "continuous_features.append('cr_hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to use a smaller sample of the data due to time constraints, use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code randomly samples 25% of the rows\n",
    "# change the frac paramter if you want a different % to sample\n",
    "# replace = False insures we won't select the same row twice\n",
    "data=data.sample(frac=.25, replace=False, ).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y from the features\n",
    "These next few steps are needed if you are not going to use the pipelines below. __If you are using the pipelines, you can skip down to that section__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def minMaxScaleContinuous(continuousList):\n",
    "    return pd.DataFrame(MinMaxScaler().fit_transform(data[continuousList])\n",
    "                             ,columns=list(data[continuousList].columns)\n",
    "                             ,index = data[continuousList].index)\n",
    "\n",
    "def createDiscreteDummies(discreteList):\n",
    "    return pd.get_dummies(data[discreteList], dummy_na = True, prefix_sep = \"::\", drop_first = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VERY IMPORTANT STEP\n",
    "You need to define which features to use in the modeling. The homework will direct you to either use all the features from the data ingestion and cleaning process or to remove some features because they are defined by LendingClub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discrete features you want to use in modeling.\n",
    "# if you want to use all the discrete features, just set discrete_features_touse = discrete_features\n",
    "discrete_features_touse =['purpose', 'term', 'verification_status', 'emp_length', 'home_ownership']\n",
    "\n",
    "# define the continuous features to use in modeling\n",
    "# if you want to use all the continuous features, just set the continuous_features_touse = continuous_features\n",
    "continuous_features_touse = ['loan_amnt', 'funded_amnt','installment','annual_inc','dti','revol_bal','delinq_2yrs','open_acc',\n",
    " 'pub_rec','fico_range_high','fico_range_low','revol_util','cr_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrete_features_touse=discrete_features\n",
    "#continuous_features_touse = continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create dummies for categorical features and concatenate with continuous features for X or predictor dataframe\n",
    "\n",
    "# Use this line of code if you do not want to scale the continuous features\n",
    "#X_continuous = data[continuous_features_touse]\n",
    "\n",
    "# use this line if you want to scale the continuous features using the MinMaxScaler in the function defined above\n",
    "X_continuous = minMaxScaleContinuous(continuous_features_touse)\n",
    "\n",
    "# create numeric dummy features for the discrete features to be used in modeling\n",
    "X_discrete = createDiscreteDummies(discrete_features_touse)\n",
    "\n",
    "#concatenate the continuous and discrete features into one dataframe\n",
    "X = pd.concat([X_continuous, X_discrete], axis = 1)\n",
    "\n",
    "# this is the target variable \n",
    "target_col = 'default'\n",
    "y=data[target_col]\n",
    "\n",
    "# create a test and train split of the transformed data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=.4)\n",
    "\n",
    "print(\"Population:\\n\",y.value_counts())\n",
    "print(\"Train:\\n\", y_train.value_counts())\n",
    "print(\"Test:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE with Cross Validation using Decision Tree\n",
    "\n",
    "This is an example of RFECV using a decision tree. You could use another tree based algorithm to select features.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html\n",
    "\n",
    "I would encourage you to experiment with other tree based algorithms such as ExtraTreesClassifier or Random Forest. The ExtraTreeClassifier fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=30, criterion=\"entropy\")\n",
    "\n",
    "rfecv = RFECV(estimator=dt, min_features_to_select = 4, cv=10, n_jobs=-1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.title(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new training and test datasets with the selected features only\n",
    "\n",
    "rfecv_selected_train = pd.DataFrame(rfecv.transform(X_train)\n",
    "                                    ,columns=list(X_train.columns[rfecv.get_support()])\n",
    "                                    ,index=X_train.index)\n",
    "\n",
    "rfecv_selected_test = pd.DataFrame(rfecv.transform(X_test)\n",
    "                                    ,columns=list(X_test.columns[rfecv.get_support()])\n",
    "                                    ,index=X_test.index)\n",
    "\n",
    "print(\"train:\", rfecv_selected_train.shape)\n",
    "print(\"test:\", rfecv_selected_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the ranking of each feature after using RFECV\n",
    "pd.Series(rfecv.ranking_, index=X_train.columns).sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important functions to save a model to pickle and load it later\n",
    "Training of these models takes time. It is advisable to save the model as a pickle after you've trained it to your satisfaction so you can use it later for comparison without having to re-train it.\n",
    "\n",
    "The code after the function defintions provides an __example__ of how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "def saveModel(filename, model):\n",
    "    joblib.dump(model, filename)\n",
    " \n",
    " \n",
    "# load the model from disk\n",
    "def loadModel(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "saveModel('dt_model', dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "svc_model = loadModel('svc_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree GridsearchCV\n",
    "This is an example grid search with cross validation using a Decision Tree Classifier. Please note the following:\n",
    "- It does not use the selected features from the RFE above.\n",
    "- To use selected features: Replace X_train with rfecv_selected_train and Replace X_test with rfecv_selected_test\n",
    "- You can adjust these parameters or add others\n",
    "- The scoring method can be changed\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'criterion'   : [\"gini\", \"entropy\"],\n",
    "              'max_depth'   : [3]\n",
    "             }\n",
    "\n",
    "print(\"Parameter grid:\\n{}\".format(parameters),'\\n')\n",
    "\n",
    "grid =  GridSearchCV(DecisionTreeClassifier(), parameters, cv=10, return_train_score=True, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# perform grid search cv on training data.  The CV algorithm divides this into training and validation\n",
    "dt_model = grid.fit(X_train, y_train)\n",
    "\n",
    "print('best params ',dt_model.best_params_,'\\n')\n",
    "print('best estimator ',dt_model.best_estimator_,'\\n')\n",
    "print('best validation score ', dt_model.best_score_,'\\n')\n",
    "print('scoring method ', dt_model.scorer_)\n",
    "\n",
    "print(\"Test set accuracy score: {:.7f}\".format(dt_model.score(X_test, y_test)))\n",
    "\n",
    "saveModel('dt_model', dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridsearchCV\n",
    "This is an example grid search with cross validation using logistic regression. Please note the following:\n",
    "- It does not use the selected features from the RFE above.\n",
    "- To use selected features: Replace X_train with rfecv_selected_train and Replace X_test with rfecv_selected_test\n",
    "- You can adjust these parameters or add others\n",
    "- The scoring method can be changed\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "parameters = {'penalty': [\"l1\"],\n",
    "              'C'      : [0.1],\n",
    "              'solver' : ['liblinear']\n",
    "             }\n",
    "\n",
    "print(\"Parameter grid:\\n{}\".format(parameters),'\\n')\n",
    "\n",
    "grid =  GridSearchCV(LogisticRegression(), parameters, cv=10, return_train_score=True, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# perform grid search cv on training data.  The CV algorithm divides this into training and validation\n",
    "lr_model = grid.fit(X_train, y_train)\n",
    "\n",
    "print('best params ',lr_model.best_params_,'\\n')\n",
    "print('best estimator ',lr_model.best_estimator_,'\\n')\n",
    "print('best validation score ', lr_model.best_score_,'\\n')\n",
    "print('scoring method ', lr_model.scorer_)\n",
    "\n",
    "print(\"Test set accuracy score: {:.7f}\".format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "saveModel('lr_model', lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridsearchCV\n",
    "This is an example grid search with cross validation using support vector machine classifier. Please note the following:\n",
    "- It does not use the selected features from the RFE above.\n",
    "- To use selected features: Replace X_train with rfecv_selected_train and Replace X_test with rfecv_selected_test\n",
    "- You can adjust these parameters or add others\n",
    "- The scoring method can be changed\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "parameters = {'C': [0.1,1.0],\n",
    "              'max_iter': [100]\n",
    "             }\n",
    "\n",
    "print(\"Parameter grid:\\n{}\".format(parameters),'\\n')\n",
    "\n",
    "grid =  GridSearchCV(LinearSVC(), parameters, cv=10, return_train_score=True, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# perform grid search cv on training data.  The CV algorithm divides this into training and validation\n",
    "svc_model = grid.fit(X_train, y_train)\n",
    "\n",
    "print('best params ',svc_model.best_params_,'\\n')\n",
    "print('best estimator ',svc_model.best_estimator_,'\\n')\n",
    "print('best validation score ', svc_model.best_score_,'\\n')\n",
    "print('scoring method ', svc_model.scorer_)\n",
    "\n",
    "print(\"Test set accuracy score: {:.7f}\".format(svc_model.score(X_test, y_test)))\n",
    "\n",
    "saveModel('svc_model', svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridsearchCV\n",
    "This is an example grid search with cross validation using random forest classifier. Please note the following:\n",
    "- It does not use the selected features from the RFE above.\n",
    "- To use selected features: Replace X_train with rfecv_selected_train and Replace X_test with rfecv_selected_test\n",
    "- You can adjust these parameters or add others\n",
    "- The scoring method can be changed\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "parameters = {'criterion' : [\"gini\", \"entropy\"],\n",
    "              'n_estimators': [50]\n",
    "             }\n",
    "\n",
    "print(\"Parameter grid:\\n{}\".format(parameters),'\\n')\n",
    "\n",
    "grid =  GridSearchCV(RandomForestClassifier(), parameters, cv=10, return_train_score=True, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# perform grid search cv on training data.  The CV algorithm divides this into training and validation\n",
    "rf_model = grid.fit(X_train, y_train)\n",
    "\n",
    "print('best params ',rf_model.best_params_,'\\n')\n",
    "print('best estimator ',rf_model.best_estimator_,'\\n')\n",
    "print('best validation score ', rf_model.best_score_,'\\n')\n",
    "print('scoring method ', rf_model.scorer_)\n",
    "\n",
    "print(\"Test set accuracy score: {:.7f}\".format(rf_model.score(X_test, y_test)))\n",
    "\n",
    "saveModel('rf_model', rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting GridsearchCV\n",
    "This is an example grid search with cross validation using gradient boosting classifier. Please note the following:\n",
    "- It does not use the selected features from the RFE above.\n",
    "- To use selected features: Replace X_train with rfecv_selected_train and Replace X_test with rfecv_selected_test\n",
    "- You can adjust these parameters or add others\n",
    "- The scoring method can be changed\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "parameters = {\"loss\" : [\"deviance\", \"exponential\"],\n",
    "              \"learning_rate\" :   [.05, .2],\n",
    "              \"n_estimators\": [50,100,500]\n",
    "             }\n",
    "\n",
    "print(\"Parameter grid:\\n{}\".format(parameters),'\\n')\n",
    "\n",
    "grid =  GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, return_train_score=True, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# perform grid search cv on training data.  The CV algorithm divides this into training and validation\n",
    "gbc_model = grid.fit(X_train, y_train)\n",
    "\n",
    "print('best params ',gbc_model.best_params_,'\\n')\n",
    "print('best estimator ',gbc_model.best_estimator_,'\\n')\n",
    "print('best validation score ', gbc_model.best_score_,'\\n')\n",
    "print('scoring method ', gbc_model.scorer_)\n",
    "\n",
    "print(\"Test set accuracy score: {:.7f}\".format(gbc_model.score(X_test, y_test)))\n",
    "\n",
    "saveModel('gbc_model', gbc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, fbeta_score, classification_report\n",
    "\n",
    "'''Function to print model accuracy information'''\n",
    "\n",
    "def printAccuracyInfo(model, X_test, y_test):\n",
    "    print(y_test.value_counts())\n",
    "    # Make predictions against the test set\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # Show the confusion matrix\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "\n",
    "    # Find the accuracy scores of the predictions against the true classes\n",
    "    print(\"accuracy: %0.3f\" % accuracy_score(y_test, pred))\n",
    "    print(\"recall: %0.3f\" % recall_score(y_test, pred, pos_label=True))\n",
    "    print(\"precision: %0.3f\" % precision_score(y_test, pred, pos_label=True))\n",
    "    print(\"f-measure: %0.3f\" % fbeta_score(y_test, pred, beta=1, pos_label=True))\n",
    "    print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use of printAccuracyInfo using gbc_model and X_test\n",
    "# note: if you trained model using rfecv_selected_train, you need to call the function with rfecv_selected_test for X_test\n",
    "printAccuracyInfo(gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "'''Function to print confusion matrix for a model\n",
    "   You may need to run this to update to scikit-learn version 0.22.1\n",
    "         !pip install -U scikit-learn --user\n",
    "'''\n",
    "\n",
    "def plotConfusionMatrix (negative_label, positive_label, model, X_test, y_test):\n",
    "    titles_options = [(\"Confusion matrix, without normalization\", None,'d'),\n",
    "                      (\"Normalized confusion matrix\", 'true','.3g')]\n",
    "    for title, normalize,val_frmt in titles_options:\n",
    "        disp = plot_confusion_matrix(model, X_test, y_test,\n",
    "                                     display_labels=[negative_label,positive_label],\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     values_format=val_frmt,\n",
    "                                     normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "        disp.ax_.set_xlabel('Predicted')\n",
    "        disp.ax_.set_ylabel('Actual')\n",
    "\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use of plotConfusionMatrix using gbc_model and X_test\n",
    "# note: if you trained model using rfecv_selected_train, you need to call the function with rfecv_selected_test for X_test\n",
    "plotConfusionMatrix('No Default', 'Default', gbc_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VERY IMPORTANT STEP\n",
    "You need to define which features to use in the modeling. The homework will direct you to either use all the features from the data ingestion and cleaning process or to remove some features because they are defined by LendingClub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discrete features you want to use in modeling.\n",
    "# if you want to use all the discrete features, just set discrete_features_touse = discrete_features\n",
    "discrete_features_touse =['purpose', 'term', 'verification_status', 'emp_length', 'home_ownership']\n",
    "\n",
    "# define the continuous features to use in modeling\n",
    "# if you want to use all the continuous features, just set the continuous_features_touse = continuous_features\n",
    "continuous_features_touse = ['loan_amnt', 'funded_amnt','installment','annual_inc','dti','revol_bal','delinq_2yrs','open_acc',\n",
    " 'pub_rec','fico_range_high','fico_range_low','revol_util','cr_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_continuous = data[continuous_features_touse]\n",
    "\n",
    "X_discrete = pd.get_dummies(data[discrete_features_touse], dummy_na = True, prefix_sep = \":\", drop_first = False)\n",
    "\n",
    "X = pd.concat([X_continuous, X_discrete], axis = 1)\n",
    "\n",
    "target_col = 'default'\n",
    "y=data[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=.4)\n",
    "\n",
    "print(\"Population:\\n\",y.value_counts())\n",
    "print(\"Train:\\n\", y_train.value_counts())\n",
    "print(\"Test:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Using Select Percentile and Random Forest Classifier\n",
    "The result of this code is a random forest classifier (rf_model) with the best percentile features selected.\n",
    "\n",
    "The rf_model can be used to make predictions with rf_model.predict(X_test) and can be passed to the accuracy visualization functions in this notebook.\n",
    "\n",
    "This pipeline can be adapted and used to create another classifier that utilizes SelectPercentile feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SelectPercentile(),\n",
    "    RandomForestClassifier())\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "params = {'selectpercentile__percentile': [10, 15, 20, 50],\n",
    "          'randomforestclassifier__max_depth': [5, 7, 9],\n",
    "          'randomforestclassifier__criterion': ['entropy', 'gini']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=10, scoring='roc_auc', n_jobs=-2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"best dataset score: \", grid.score(X_test, y_test)) \n",
    "print(\"best parameters: \", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=grid.best_estimator_\n",
    "rf_model.steps\n",
    "\n",
    "saveModel('rf_model', rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Using Random Forest RFECV and SVM Classifier with StandardScaler\n",
    "The result of this code is a SVM classifier (svc_model) with the best scaling and features selected using random forest.\n",
    "\n",
    "The svc_model can be used to make predictions with svc_model.predict(X_test) and can be passed to the accuracy visualization functions in this notebook.\n",
    "\n",
    "This pipeline can be adapted and used to create another classifier that needs scaling and utilizes RFECV feature selection.\n",
    "\n",
    "You can also replace the StandardScaler with another scaler such as MinMaxScaler. To do that you would have to make sure the parameters are changed as well as StandardScaler() replaced with MinMaxScaler in the pipeline.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "            (StandardScaler(),continuous_features_touse,))\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    preprocess,\n",
    "    RFECV(estimator=RandomForestClassifier(random_state=1, n_estimators=100)),\n",
    "    LinearSVC())\n",
    "\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "''' These are just example parameter settings. You can change these parameters or add others.\n",
    "    The grid search uses a scoring method of roc_auc. You can change that to another scoring method.\n",
    "'''\n",
    "\n",
    "params = {'columntransformer__standardscaler__with_mean': [True, False],\n",
    "          'rfecv__estimator__criterion': ['gini', 'entropy'],\n",
    "          'linearsvc__C': [0.1,1.0],\n",
    "          'linearsvc__max_iter': [100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=10, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"best dataset score: \", grid.score(X_test, y_test)) \n",
    "print(\"best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model=grid.best_estimator_\n",
    "svc_model.steps\n",
    "\n",
    "saveModel('svc_model', svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZE PERFORMANCE SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create ROC curve for one model\n",
    "__NOTE:__ This will not work with SVC models, as they do not have a predict_proba function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_1 (model, model_name, X_test, y_test):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # predict probabilities\n",
    "    cf_probs = model.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    cf_probs = cf_probs[:, 1]\n",
    "\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    cf_auc = roc_auc_score(y_test, cf_probs)\n",
    "\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print(model_name,': ROC AUC=%.3f' % (cf_auc))\n",
    "\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "    cf_fpr, cf_tpr, _ = roc_curve(y_test, cf_probs)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(cf_fpr, cf_tpr, marker='.', label=model_name)\n",
    "\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example ROC plot for random forest model using X_test.\n",
    "## if you want to use selected features, pass rfecv_selected_test instead of X_test\n",
    "plot_roc_curve_1(rf_model, 'Random Forest', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to plot multiple ROC curves for multiple models\n",
    "__NOTE:__ This will not work with SVC models, as they do not have a predict_proba function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_roc_curve(model, model_name, X_test, y_test):\n",
    "    '''\n",
    "    Plot ROC curve.\n",
    "    \n",
    "    INPUTS:\n",
    "    - model object\n",
    "    - model name\n",
    "    - X_test\n",
    "    - y_test\n",
    "    ''' \n",
    "    # predict probabilities\n",
    "    cf_probs = model.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    cf_probs = cf_probs[:, 1]\n",
    "\n",
    "    # calculate scores\n",
    "    cf_auc = round(roc_auc_score(y_test, cf_probs),3)\n",
    "\n",
    "    # calculate roc curve\n",
    "    cf_fpr, cf_tpr, _ = roc_curve(y_test, cf_probs)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(cf_fpr, cf_tpr, marker='.', label= '{}, ROC AUC {}'.format(model_name, cf_auc))\n",
    "    \n",
    "def plot_roc_curves(classifiers, X_test, y_test):\n",
    "        \n",
    "    # Plot roc curve for each classifier\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for name, classifier in classifiers.items():\n",
    "        plot_roc_curve(classifier, name, X_test, y_test)\n",
    "            \n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "    ns_auc = round(roc_auc_score(y_test, ns_probs),3)\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "    \n",
    "    # plot the roc curve for no skill model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label= '{}, ROC AUC {}'.format('No Skill', ns_auc))\n",
    "\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a dictionary of classifiers to plot\n",
    "classifiers = {\n",
    "    \"RF\"  : rf_model, \n",
    "    \"GBC\" : gbc_model,\n",
    "    \"DT\"  : dt_model,\n",
    "    \"LR\"  : lr_model\n",
    "}\n",
    "\n",
    "## example ROC plot for a dictionary of models using X_test.\n",
    "## if you want to use selected features, pass rfecv_selected_test instead of X_test\n",
    "plot_roc_curves(classifiers, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot ROC curve for a single model using Sklearn new functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotNewROCCurve(model, X_test, y_test):\n",
    "    disp = plot_roc_curve(model, X_test, y_test)\n",
    "    disp.figure_.suptitle(\"ROC Curve\")\n",
    "    disp.ax_.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example new ROC plot for SVM model using X_test.\n",
    "## if you want to use selected features, pass rfecv_selected_test instead of X_test\n",
    "plotNewROCCurve(svc_model.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot Precision Recall Curve for a single model using Sklearn new functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotNewPrecisionRecall(model, X_test, y_test):\n",
    "    disp = plot_precision_recall_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example new Precision Recall plot for SVC model using X_test.\n",
    "## if you want to use selected features, pass rfecv_selected_test instead of X_test\n",
    "plotNewPrecisionRecall(svc_model.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to plot multiple Precision Recall curves for multiple models\n",
    "__NOTE:__ This will not work with SVC models, as they do not have a predict_proba function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_precision_recall_curve(model, model_name, X_test, y_test):\n",
    "    '''\n",
    "    Plot Precision Recall curve.\n",
    "    \n",
    "    INPUTS:\n",
    "    - model object\n",
    "    - model name\n",
    "    - X_test\n",
    "    - y_test\n",
    "    ''' \n",
    "    # predict probabilities\n",
    "    cf_probs = model.predict_proba(X_test)\n",
    "    pred = model.predict(X_test) \n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    cf_probs = cf_probs[:, 1]\n",
    "\n",
    "    # predict class values\n",
    "    yhat = model.predict(X_test)\n",
    "    cf_precision, cf_recall, _ = precision_recall_curve(y_test,cf_probs)\n",
    "    cf_f1, cf_auc = f1_score(y_test, yhat), auc(cf_recall, cf_precision)\n",
    "    cf_f1 = round(cf_f1,3)\n",
    "    cf_auc = round(cf_auc,3)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(cf_recall, cf_precision, marker='.', label = '{}, f1 {}, auc {}'.format(model_name, cf_f1, cf_auc))\n",
    "    \n",
    "def plot_precision_recall_curves(classifiers, X_test, y_test):        \n",
    "    # Plot roc curve for each classifier\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    \n",
    "    for name, classifier in classifiers.items():\n",
    "        plot_precision_recall_curve(classifier, name, X_test, y_test)\n",
    "\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Correlation Using Spearman's Correlation\n",
    "\n",
    "Use the following function to find the rank correlation between the Lending Club grades and the probability of default by your model. \n",
    "\n",
    "You can call the function with those values using your test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def getCorrelation (grades, scores):\n",
    "    coef, p = spearmanr(grades, scores)\n",
    "    print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
