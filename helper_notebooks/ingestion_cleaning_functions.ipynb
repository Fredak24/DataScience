{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion and Cleaning Functions\n",
    "\n",
    "This notebook contains functions and templates useful for:\n",
    "  - Ingesting data downloaded from the LendingClub website\n",
    "  - Compare it to an earlier cut of the data, to identify columns that are not changing\n",
    "  - Output a combined dataset ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sys import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(x):\n",
    "    '''\n",
    "    This function returns True if x is an integer, and False otherwise\n",
    "    '''\n",
    "    try:\n",
    "        return (int(x) == float(x))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_files(directory):\n",
    "    '''\n",
    "    This function will ingest every file in the specified directory\n",
    "    into a pandas dataframe. It will return a dictionary containing\n",
    "    these dataframes, keyed by the file name.\n",
    "    \n",
    "    We assume the directory contains files directly downloaded from\n",
    "    MC, and *only* those files. Thus, we assume the files are zipped\n",
    "    (pd.read_csv can read zipped files) and we assume the first line\n",
    "    in each file needs to be skipped\n",
    "    \n",
    "    Note that each file will be read *without* formatting\n",
    "    '''\n",
    "    \n",
    "    # If the directory has no trailing slash, add one\n",
    "    if directory[-1] != \"/\":\n",
    "        directory = directory + \"/\"\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    output = {}\n",
    "    \n",
    "    print(\"Directory \" + directory + \" has \" + str(len(all_files)) + \" files:\")\n",
    "    for i in all_files:\n",
    "        print(\"    Reading file \" + i)\n",
    "        output[i] = pd.read_csv(directory + i, dtype = str, skiprows = 1)\n",
    "        \n",
    "        # Some of the files have \"summary\" lines that, for example\n",
    "        # read \"Total number of loans number in Policy 1: .....\"\n",
    "        # To remove those lines, find any lines with non-integer IDs\n",
    "        # and remove them\n",
    "        invalid_rows = (output[i].id.apply( lambda x : is_integer(x) == False ))\n",
    "        if invalid_rows.sum() > 0:\n",
    "            print(\"        Found \" + str(invalid_rows.sum()) + \" invalid rows which were removed\")\n",
    "            output[i] = output[i][invalid_rows == False]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the directories that contain the files downloaded in 2017 and 2019\n",
    "dir_2017 = \"/\".join([\"Data\"] + [\"1712_download\"] )\n",
    "dir_2019 = \"/\".join([\"Data\"] + [\"1912_download\"] )\n",
    "\n",
    "# Ingest the set of files downloaded in 2017 and then the files downloaded in 2019\n",
    "files_2017 = ingest_files(dir_2017)\n",
    "files_2019 = ingest_files(dir_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the two sets of files have the same number of files\n",
    "# with the same names\n",
    "assert len(files_2017) == len(files_2019)\n",
    "assert sorted(files_2017) == sorted(files_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each set of files has the same loan IDs in both instances\n",
    "for i in files_2017:\n",
    "    assert sorted(files_2017[i].id) == sorted(files_2019[i].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017 = pd.concat(files_2017.values()).reset_index(drop = True)\n",
    "data_2019 = pd.concat(files_2019.values()).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017 dataset has an additional column\n",
    "data_2017.drop(['disbursement_method'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_2017.shape)\n",
    "print(data_2019.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=set(data_2017.columns)\n",
    "B=set(data_2019.columns)\n",
    "\n",
    "A-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the loan IDs are a unique key\n",
    "assert len(set(data_2019.id)) == len(data_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Static Columns\n",
    "Find columns that were static in both sets of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data_2017.columns)\n",
    "\n",
    "# We verified every file had the same columns, but double check just in case\n",
    "assert sorted(columns) == sorted(data_2019.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the files. Because we have previously confirmed that each\n",
    "# set of files contained the same sets of IDs *and* that these\n",
    "# IDs were unique, the two files can be joined safely\n",
    "\n",
    "# Just in case, check the datasets have the same number of rows\n",
    "n_rows = len(data_2017)\n",
    "assert n_rows == len(data_2019)\n",
    "\n",
    "# Merge them\n",
    "combined = pd.merge(data_2017, data_2019, how = 'inner', on=\"id\", suffixes=('_x', '_y'))\n",
    "\n",
    "# Ensure the merged dataset has the same number of rows\n",
    "assert n_rows == len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each column, and find what percentage of the values in\n",
    "# that column are identical in the two datasets\n",
    "static_perc = {}\n",
    "\n",
    "# Remove the ID column first\n",
    "columns = [i for i in columns if i != \"id\"]\n",
    "\n",
    "for i in columns:\n",
    "    combined[i+\"_comp\"] = (combined[i+\"_x\"] == combined[i+\"_y\"]) | (combined[i+\"_x\"].isnull() & combined[i+\"_y\"].isnull())\n",
    "    static_perc[i] = combined[i+\"_comp\"].sum()*100.0/len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_perc = pd.DataFrame([ [i, static_perc[i]] for i in static_perc], columns=[\"column\", \"perc_equal\"]).\\\n",
    "                                                    sort_values(\"perc_equal\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the columns we want to pick for our model are in\n",
    "# the above, and check how consistent they've been\n",
    "assert set(cols_to_pick) - set(static_perc.column) - set([\"id\"]) == set()\n",
    "\n",
    "static_perc[static_perc.column.isin(cols_to_pick)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider the columns that were not consistent in both datasets\n",
    "\n",
    "# First, make sure the columns in which int_rate and installment\n",
    "# are different are the same columns\n",
    "assert (combined[\"int_rate_comp\"] != combined[\"installment_comp\"]).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the differences in int_rate and installment are the same. Let's look at them\n",
    "combined[combined.int_rate_comp == False][[\"id\", \"issue_d_x\", \"issue_d_y\", \"term_x\", \"term_y\", \"int_rate_x\", \"int_rate_y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories that contain the full dataset files downloaded in 2019\n",
    "dir_full = \"/\".join([\"Data\"] + [\"full_dataset\"] )\n",
    "\n",
    "# Ingest the set of files\n",
    "files_full = ingest_files(dir_full)\n",
    "\n",
    "final_data = pd.concat(files_full.values()).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns of interest\n",
    "final_data = final_data[cols_to_pick].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting with \" + str(len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typecast Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in float_cols:\n",
    "    final_data[i] = final_data[i].astype(float)\n",
    "    \n",
    "def clean_perc(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(x.strip()[:-1])\n",
    "for i in perc_cols:\n",
    "    final_data[i] = final_data[i].apply( clean_perc )\n",
    "    \n",
    "def clean_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strptime( x, \"%b-%Y\").date()\n",
    "for i in date_cols:\n",
    "    final_data[i] = final_data[i].apply( clean_date )\n",
    "    \n",
    "for i in cat_cols:\n",
    "    final_data.loc[final_data[i].isnull(), i] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names of the four returns we'll be calculating\n",
    "ret_cols = [\"ret_PESS\", \"ret_OPT\", \"ret_INTa\", \"ret_INTb\", \"ret_INTc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows for loans that were paid back on the days\n",
    "# they were issued\n",
    "final_data['loan_length'] = (final_data.last_pymnt_d - final_data.issue_d) / np.timedelta64(1, 'M')\n",
    "\n",
    "n_rows = len(final_data)\n",
    "\n",
    "final_data = final_data[final_data.loan_length != 0]\n",
    "\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return Method 2 (pessimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the return using a simple annualized profit margin\n",
    "# Pessimistic fefinition (method 2)\n",
    "\n",
    "final_data['term_num'] = final_data.term.str.extract('(\\d+)',expand=False).astype(int)\n",
    "final_data['ret_PESS'] = ( (final_data.total_pymnt - final_data.funded_amnt) \n",
    "                                            / final_data.funded_amnt ) * (12 / final_data['term_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return Method 1 (optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that if a loan gives a positive return, we can\n",
    "# immediately find a similar loan to invest in; if the loan\n",
    "# takes a loss, we use method 2 to compute the return\n",
    "\n",
    "final_data['ret_OPT'] = ( (final_data.total_pymnt - final_data.funded_amnt)\n",
    "                                            / final_data.funded_amnt ) * (12 / final_data['loan_length'])\n",
    "final_data.loc[final_data.ret_OPT < 0,'ret_OPT'] = final_data.ret_PESS[final_data.ret_OPT < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return Method 3 (re-investment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_method_3(T, i):\n",
    "    '''\n",
    "    Given an investment time horizon (in months) and re-investment\n",
    "    interest rate, calculate the return of each loan\n",
    "    '''\n",
    "    \n",
    "    # Assuming that the total amount paid back was paid at equal\n",
    "    # intervals during the duration of the loan, calculate the\n",
    "    # size of each of these installment\n",
    "    actual_installment = (final_data.total_pymnt - final_data.recoveries) / final_data['loan_length']\n",
    "\n",
    "    # Assuming the amount is immediately re-invested at the prime\n",
    "    # rate, find the total amount of money we'll have by the end\n",
    "    # of the loan\n",
    "    cash_by_end_of_loan = actual_installment * (1 - pow(1 + i, final_data.loan_length)) / ( 1 - (1 + i) )\n",
    "    \n",
    "    cash_by_end_of_loan = cash_by_end_of_loan + final_data.recoveries\n",
    "    \n",
    "    # Assuming that cash is then re-invested at the prime rate,\n",
    "    # with monthly re-investment, until T months from the start\n",
    "    # of the loan\n",
    "    remaining_months = T - final_data['loan_length']\n",
    "    final_return = cash_by_end_of_loan * pow(1 + i, remaining_months)\n",
    "\n",
    "    # Find the percentage return\n",
    "    return( (12/T) * ( ( final_return - final_data['funded_amnt'] ) / final_data['funded_amnt'] ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_columns():\n",
    "    '''\n",
    "    This function visualizes all columns\n",
    "      - Box-and-whisker plots for continuous variables\n",
    "      - Lists of distinct values for categorical columns\n",
    "      - A timeline density for dates\n",
    "    '''\n",
    "    \n",
    "    # FLoat columns\n",
    "    for i in float_cols + perc_cols + ret_cols:\n",
    "        seaborn.boxplot(final_data[i])\n",
    "\n",
    "        # Print the three highest values\n",
    "        highest_vals = sorted(final_data[i], reverse=True)[:3]\n",
    "        smallest_val = min(final_data[i])\n",
    "        plt.text(smallest_val, -0.3, highest_vals[0])\n",
    "        plt.text(smallest_val, -0.2, highest_vals[1])\n",
    "        plt.text(smallest_val, -0.1, highest_vals[2])\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    # Categorical columns \n",
    "    for i in cat_cols:\n",
    "        print(i)\n",
    "        print(str(len(set(final_data[i]))) + \" distinct values\")\n",
    "        print(final_data[i].value_counts())\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Date columns\n",
    "    for i in date_cols:\n",
    "        final_data[final_data[i].isnull() == False][i].apply(lambda x : str(x.year) +\n",
    "                                                \"-\" + str(x.month)).value_counts(ascending = True).plot()\n",
    "        plt.title(i + \" (\" + str(final_data[i].isnull().sum()) + \" null values)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are quite a few outliers, but the two most obvious\n",
    "# ones to remove are in annual_inc, revol_util Remove these.\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.annual_inc < ??]\n",
    "final_data = final_data[final_data.revol_util < ??]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include loans isssued since 2009\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.issue_d >= datetime.date(2009, 1, 1)]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data again\n",
    "visualize_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with null values. We allow cateogrical variables to be null\n",
    "# OTHER than grade, which is a particularly important categorical.\n",
    "# All non-categorical variables must be non-null, and we drop\n",
    "# rows that do not meet this requirement\n",
    "required_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\n",
    "required_cols.add(\"grade\")\n",
    "\n",
    "n_rows = len(final_data)\n",
    "final_data.dropna(subset = required_cols ,inplace=True)\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the percentage of loans by grade, the default by grade,\n",
    "# and the return of each grade\n",
    "perc_by_grade = (final_data.grade.value_counts()*100/len(final_data)).sort_index()\n",
    "default_by_grade = final_data.groupby(\"grade\").apply(lambda x : (x.loan_status != \"Fully Paid\").sum()*100/len(x) )\n",
    "ret_by_grade_OPT = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.ret_OPT)*100 )\n",
    "ret_by_grade_PESS = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.ret_PESS)*100 )\n",
    "ret_by_grade_INTa = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.ret_INTa)*100 )\n",
    "ret_by_grade_INTb = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.ret_INTb)*100 )\n",
    "ret_by_grade_INTc = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.ret_INTc)*100 )\n",
    "int_rate_by_grade = final_data.groupby(\"grade\").apply(lambda x : np.mean(x.int_rate))\n",
    "\n",
    "combined = pd.DataFrame(perc_by_grade)\n",
    "combined['default'] = default_by_grade\n",
    "combined['int_rate'] = int_rate_by_grade\n",
    "combined['return_OPT'] = ret_by_grade_OPT\n",
    "combined['return_PESS'] = ret_by_grade_PESS\n",
    "combined['return_INTa'] = ret_by_grade_INTa\n",
    "combined['return_INTb'] = ret_by_grade_INTb\n",
    "combined['return_INTc'] = ret_by_grade_INTc\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness & Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "print('kurtosis is: {}'.format(kurtosis(final_data.int_rate)))\n",
    "\n",
    "print('skewness is: {}'.format(skew(final_data.int_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [final_data, discrete_features, continuous_features, ret_cols], open(pickle_file, \"wb\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
